# This yaml contains the basic architecture overrides that need to apply to the
# `base_config.yaml` so we recover SmolLM2-135M architecture.
# We can use this small model to play with pre-training while the arch team
# prepares architecture novelties.

# Throughput info:
# - With   1GPUs: 122k t/s/g.
# - With   8GPUs: 119k t/s/g.
# - With 2x8GPUs: 81k  t/s/g.  (Using optimizer/zero_stage:0), otherwise it has bad peformance.

general:
  run: smollm2-135M
model:
  init_method:
    std: 0.041666666666666664
  model_config:
    hidden_size: 576
    initializer_range: 0.041666666666666664
    intermediate_size: 1536
    tie_word_embeddings: true
    vocab_size: 49152
optimizer:
  learning_rate_scheduler:
    learning_rate: 0.003
    lr_decay_starting_step: 1600000
    lr_decay_steps: 400000
    lr_decay_style: linear
    lr_warmup_steps: 2000
    lr_warmup_style: linear
    min_decay_lr: 0
tokens:
  train_steps: 2000000
  global_batch_size: 512  # note that `global_batch_size` is not a nanotron.config key, the `unify_config.py` translates this to `batch_accumulation_per_replica`.
  micro_batch_size: 32  # Max MBS before OOM.
